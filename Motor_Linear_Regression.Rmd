---
title: "Linear Regression"
author: "Silva, Eder H N"
date: "2022-12-27"
#output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(latex2exp)
```

# Correspondence Between Force and Command Input
Experiments were carried out to characterize the relations between the force generated by the main motor and the command input. For this purpose, a wire rope was used to connect the beam to a load cell a device used to measure the force. The force exerted by the main motor, drove by $u$, actuated in the load cell through the wire rope the corresponding data were collected and stored.


```{r Load data , echo=TRUE, warning=FALSE}
data <- read_csv("M2_dados1.csv")
data <- as.data.frame(data)
colnames(data)=c("sample","force","u")
head(data)
```

# Standardized

```{r echo=TRUE}
plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",bty="l",lwd=2)
par(new=TRUE)
plot(x=data[,3],y=data[,2],type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=2)

filtered_rows <- data$u == 8
rows <- data[filtered_rows,]

hist(rows[,2],xlab="Force (N)",main=TeX('$u = 8$'),col="#ADD8E6")

boxplot(rows[,2],main=TeX('$u = 8$'),col="#ADD8E6")

shapiro.test(rows[,2])

plot(rows[,1],rows[,2],type="l",col="darkblue",lwd=1,ylab="Force (N)",xlab="Command Input (u)",main=TeX('$u = 8$'),bty="l")
abline(h=median(rows[,2]),lty=2,col="red") 
abline(h=(median(rows[,2])+sd(rows[,2])),lty=2,col="black") 
abline(h=(median(rows[,2])-sd(rows[,2])),lty=2,col="black") 
legend("topright",legend = c('data','median','standard deviation'),col = c("darkblue","red","black"),lty = c(1,2,2), lwd=2)
```


```{r echo=TRUE}
uni <- unique(data[,3])
modelagem = matrix(data=0, ncol=4, nrow=length(uni))
colnames(modelagem)=c("u","Mean","Median","Length")

for (i in 1:length(uni)) {
  
  #Seleciona as linhas com tensões iguais ao valor do vetor uni
  filtered_rows <- data$u == uni[i]
  modelagem[i,1] <- uni[i]
  #Guarda as linhas com tensões iguais no vetor rows 
  rows <- data[filtered_rows,]
  
  #Guarda a média dos valores de força 
  modelagem[i,2]  <- mean(rows[,2])
  
  #Guarda a mediana dos valores de força 
  modelagem[i,3] <- median(rows[,2])
  
  #Guarda a moda dos valores de força 
  modelagem[i,4] <- length(rows[,2])
}
head(modelagem)
```

# Using scatter plots to explore relationships
 Before looking ahead to predicting a value of force by using a value of $u$, first establish the legitimate reason to using a linear function to make that prediction will actually work well.

 In order to achieve both of these important steps, first plot the data in a pairwise fashion so you can visually look for a relationship; then need to somehow quantify that relationship in terms of how well those points follow a linear function.
 
```{r echo=TRUE}
plot(x=data[,3],y=data[,2],ylim=c(-.05,.01),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty = "l")
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)
```

 After displaying the data using a scatter plot, the next step is to find a statistic that quantifies the relationship somehow. The correlation coefficient (also known as Pearson’s correlation coefficient) measures the strength and direction of the linear relationship between two quantitative variables $u$ and $F$.
 
```{r Pearson’s, echo=TRUE}
cor.test(x=modelagem[,1],y=modelagem[,3])
```

# Building a Simple Linear Regression Model

$$F_i=\beta_0 + \beta_1 u_i + \beta_2 u_i^2 + \beta_3 u_i^3 + \varepsilon_i$$

```{r Linear Regression, echo=TRUE}
x <- modelagem[,1]
y <- modelagem[,3]

X <- x
XSQ <- x**2
XCUB <- x**3

model=lm(y~1+X+XSQ+XCUB)
anova(model)
summary(model)
func <- model$coefficients
print(func)

x=0:10
y=as.numeric(func[1])+as.numeric(func[2])*x+as.numeric(func[3])*x**2+as.numeric(func[4])*x**3

plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty="l")
par(new=TRUE)
plot(x,y,type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="red",lwd=2)
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)
legend("topleft",legend = c('Measured','Model'),col = c("darkblue","red"),lty = c(0, 1), lwd=2, pch =c(16, NA))
```

### Using $r^2$ to measure model fit

 One important way to assess how well the model fits is to measure the value of $r^2$, where $r$ is the correlation coefficient. Statisticians measure how well a model fits by looking at what percentage of the variability in $F$ is explained by the model.

# Finding and exploring the residuals

After you’ve established a relationship between $u$ and $F$ and have come up with an equation of a linear that represents that relationship, the job is not done. (Many researchers erringly stop here, so I’m depending on you to break the cycle on this!) 

But the most important job remains to be completed: checking to be sure that the conditions of the model are truly met and that the model fits well in more specific ways than the scatter plot and correlation measure. This section presents methods for defining and assessing the fit of a simple linear regression model.

### Two major conditions must be met before you apply a simple linear regression model to a data set:

- The y’s have to have a normal distribution for each value of x.
- The y’s have to have a constant amount of spread (standard deviation) for each value of x.

### Finding the residuals

A residual is the difference between the observed value of force and the predicted value of $F$. Specifically, for any data point, takes observed $F$-value (from the data) and subtract the expected $F$-value (from the line). If the residual is large, the linear function doesn’t fit well in that spot. If the residual is small, the line fits well in that spot.

```{r Residuals,  echo=TRUE}
error=resid(model)
```

To check to see whether the $F$-values come from a normal distribution. The residuals are a data set just like any other data set, so can find their mean and standard deviation. 
 
 **Checking normality**
 
```{r , echo=TRUE}
summary(error)
sd(error)

shapiro.test(error)

hist(error,col="#ADD8E6",main="Histogram of the Residuals",xlab="Residuals")

qqnorm(error,pch=19,col="darkblue",main="Normal Probability Plot of the Residuals",bty = "l")
qqline(error,col="red")

```

 As the condition of normality is met, then residual plot lots of residuals close to zero. The residuals also occur at random some above the line, some below the line.

```{r , echo=TRUE}
plot(error,pch=19,col="darkblue",main="Residuals versus the Order of the Data",xlab="Observation Order",ylab="Residual",bty = "l")
par(new=TRUE)
plot(error,type="l",axes=FALSE,ann=FALSE)
abline(h=0,lty=2,col="red") 
```

# Reference
```{r , echo=TRUE}
#Citing
citation()
citation("readr")
citation("rjson")
citation("latex2exp")
```
