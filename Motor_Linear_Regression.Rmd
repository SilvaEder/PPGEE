---
title: "Linear Regression"
author: "Silva, Eder H N"
date: "2022-12-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(readr)
library(rjson)
library(latex2exp)
```

# Correspondence Between Force and Command Input
Experiments were carried out to characterize the relations between the force generated by the main motor and the command input. For this purpose, a wire rope was used to connect the beam to a load cell a device used to measure the force. The force exerted by the main motor, drove by $u$, actuated in the load cell through the wire rope the corresponding data were collected and stored.

```{r Load data , echo=TRUE, warning=FALSE}
data <- read_csv("CSV/motor0_data1.csv")
data <- as.data.frame(data)
colnames(data)=c("sample","force","u")
head(data)
```

# Motor 0 standardized data
```{r echo=TRUE}
plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",bty="l",lwd=2)
par(new=TRUE)
plot(x=data[,3],y=data[,2],type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=2)
```

### Example sample record of $u = 8$

```{r echo=TRUE}
filtered_rows <- data$u == 8
rows <- data[filtered_rows,]

hist(rows[,2],xlab="Force (N)",main=TeX('$u = 8$'),col="#ADD8E6")

boxplot(rows[,2],main=TeX('$u = 8$'),horizontal=TRUE,col="#ADD8E6")

shapiro.test(rows[,2])

plot(rows[,1],rows[,2],type="l",col="darkblue",lwd=1,ylab="Force (N)",xlab="Command Input (u)",main=TeX('$u = 8$'),bty="l")
abline(h=median(rows[,2]),lty=2,col="red") 
abline(h=(median(rows[,2])+sd(rows[,2])),lty=2,col="black") 
abline(h=(median(rows[,2])-sd(rows[,2])),lty=2,col="black") 
legend("topright",legend = c('data','median','standard deviation'),col = c("darkblue","red","black"),lty = c(1,2,2), lwd=2)
```

According to the previous figure, there are indications the condition of normality is not met. Therefore, the mean may not be a good representation of the data, which is why the median was used.

```{r echo=TRUE}
uni <- unique(data[,3])
modelagem = matrix(data=0, ncol=4, nrow=length(uni))
colnames(modelagem)=c("u","Mean","Median","Length")

for (i in 1:length(uni)) {
  
  filtered_rows <- data$u == uni[i]
  modelagem[i,1] <- uni[i]
  rows <- data[filtered_rows,]
  modelagem[i,2]  <- mean(rows[,2])
  modelagem[i,3] <- median(rows[,2])
  modelagem[i,4] <- length(rows[,2])
}
head(modelagem)
```

It can be seen that the samples do not have the same size, but as many samples as possible were used, seeking a good representation of the data.

# Using scatter plots to explore relationships
 Before looking ahead to predicting a value of force by using a value of $u$, first establish the legitimate reason to using a linear function to make that prediction will actually work well.

 In order to achieve both of these important steps, first plot the data in a pairwise fashion so you can visually look for a relationship; then need to somehow quantify that relationship in terms of how well those points follow a linear function.
 
```{r echo=TRUE}
plot(x=data[,3],y=data[,2],ylim=c(-.05,.01),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty = "l")
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)
```
 
 Each circle represents the statistical median of more than one-thousand points.
 
 After displaying the data using a scatter plot, the next step is to find a statistic that quantifies the relationship somehow. The correlation coefficient (also known as Pearson’s correlation coefficient) measures the strength and direction of the linear relationship between two quantitative variables $u$ and $y$.
 
```{r Pearson’s, echo=TRUE}
cor.test(x=modelagem[,1],y=modelagem[,3])
```

# Building a Simple Linear Regression Model

$$y_i=\beta_0 + \beta_1 u_i + \beta_2 u_i^2 + \beta_3 u_i^3 + \varepsilon_i$$

```{r Linear Regression, echo=TRUE}
u <- modelagem[,1]
y <- modelagem[,3]

X <- u
XSQ <- u**2
XCUB <- u**3

model=lm(y~1+X+XSQ+XCUB)
anova(model)
summary(model)
summ=summary(model)
func <- model$coefficients
print(func)

u=0:10
y=as.numeric(func[1])+as.numeric(func[2])*u+as.numeric(func[3])*u**2+as.numeric(func[4])*u**3

plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty="l")
par(new=TRUE)
plot(u,y,type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="red",lwd=2)
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)
legend("topleft",legend = c('Measured','Model'),col = c("darkblue","red"),lty = c(0, 1), lwd=2, pch =c(16, NA))
```

### Using $r^2$ to measure model fit

 One important way to assess how well the model fits is to measure the value of $r^2$, where $r$ is the correlation coefficient. Statisticians measure how well a model fits by looking at what percentage of the variability in $\hat{y}$ is explained by the model.
 
```{r ,  echo=TRUE}
summ$r.squared
```

# Finding and exploring the residuals

After you’ve established a relationship between $u$ and $y$ and have come up with an equation of a linear that represents that relationship, the job is not done. (Many researchers erringly stop here, so I’m depending on you to break the cycle on this!) 

But the most important job remains to be completed: checking to be sure that the conditions of the model are truly met and that the model fits well in more specific ways than the scatter plot and correlation measure. This section presents methods for defining and assessing the fit of a simple linear regression model.

### Finding the residuals

A residual is the difference between the observed value of force and the predicted value of $\hat{y}$. Specifically, for any data point, takes observed $y$ (from the data) and subtract the expected $\hat{y}$. If the residual is large, the linear function doesn’t fit well in that spot. If the residual is small, the line fits well in that spot.

```{r Residuals,  echo=TRUE}
error=resid(model)
```

The conditions for regression concentrate on the error terms, or residuals. The residuals are the amount that's left over after the model has been fit.
They represent the difference between the actual value of $y$ observed in the data set and the estimated value of $\hat{y}$ based on the model. The conditions of the regression model are the following:

### Noting the conditions

- **The residuals have a normal distribution with mean zero $\varepsilon_i \backsim N(0,\sigma^2)$.**
 
```{r , echo=TRUE}
summary(error)
sd(error)

shapiro.test(error)

hist(error,col="#ADD8E6",main="Histogram of the Residuals",xlab="Residuals")

boxplot(error,main="Residuals",col="#ADD8E6",horizontal=TRUE)

qqnorm(error,pch=19,col="darkblue",main="Normal Probability Plot of the Residuals",bty = "l")
qqline(error,col="red")
```

 As the condition of normality is met, then residual plot lots of residuals close to zero. The residuals also occur at random some above the line, some below the line.

- **The residuals have the same variance for each fitted (predicted) value of $\hat{y}$, $Var(\varepsilon_i)=\sigma^2$.**

```{r , echo=TRUE}
var(error)
```

- **The residuals are independent (don’t affect each other).**

```{r , echo=TRUE}
plot(error,pch=19,col="darkblue",main="Residuals versus the Order of the Data",xlab="Observation Order",ylab="Residual",bty = "l")
par(new=TRUE)
plot(error,type="l",axes=FALSE,ann=FALSE)
abline(h=0,lty=2,col="red") 
legend("topright",legend = c('residual','median'),col = c("darkblue","red"),lty = c(0,2), lwd=2, pch =c(16,NA))
```

# Writing into JSON file

```{r , echo=TRUE}
# creating the list
list1 <- vector(mode="list", length=2)
list1[[1]] <- c("Intercept", "u", "u^2", "u^3")
list1[[2]] <- c(as.numeric(func[1]), as.numeric(func[2]),as.numeric(func[3]), as.numeric(func[4]))

# creating the data for JSON file
jsonData <- toJSON(list1)

# writing into JSON file
write(jsonData, "JSON/motor0.json") 

# Give the created file name to the function
motor0 <- fromJSON(file = "JSON/motor0.json")

# Print the result
print(motor0)

list2 <- vector(mode="list", length=1)
list2[[1]] <- error
jsonData <- toJSON(list2)
write(jsonData, "JSON/motor0_resi.json")

list3 <- vector(mode="list", length=1)
list3[[1]] <- modelagem[,3]
jsonData <- toJSON(list3)
write(jsonData, "JSON/motor0_median.json")
```

# Motor 1 standardized data

```{r , echo=TRUE}
data <- read_csv("CSV/motor1_data1.csv")
data <- as.data.frame(data)
colnames(data)=c("sample","force","u")
head(data)

plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",bty="l",lwd=2)
par(new=TRUE)
plot(x=data[,3],y=data[,2],type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=2)

uni <- unique(data[,3])
modelagem = matrix(data=0, ncol=4, nrow=length(uni))
colnames(modelagem)=c("u","Mean","Median","Length")

for (i in 1:length(uni)) {

  filtered_rows <- data$u == uni[i]
  modelagem[i,1] <- uni[i]
  rows <- data[filtered_rows,]
  modelagem[i,2]  <- mean(rows[,2])
  modelagem[i,3] <- median(rows[,2])
  modelagem[i,4] <- length(rows[,2])
}
head(modelagem)

plot(x=data[,3],y=data[,2],ylim=c(-.05,.01),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty = "l")
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)

cor.test(x=modelagem[,1],y=modelagem[,3])


u <- modelagem[,1]
y <- modelagem[,3]

X <- u
XSQ <- u**2
XCUB <- u**3

model=lm(y~1+X+XSQ+XCUB)
anova(model)
summary(model)
summ=summary(model)
func <- model$coefficients
print(func)

u=0:10
y=as.numeric(func[1])+as.numeric(func[2])*u+as.numeric(func[3])*u**2+as.numeric(func[4])*u**3

plot(x=data[,3],y=data[,2],ylim=c(min(data[,2]),max(data[,2])),type="n",ylab="Force (N)",xlab="Command Input (u)",main="Scatter Plot",lwd=2,bty="l")
par(new=TRUE)
plot(u,y,type="l",ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="red",lwd=2)
par(new=TRUE)
plot(x=modelagem[,1],y=modelagem[,3],pch=19,ylim=c(min(data[,2]),max(data[,2])),axes=FALSE,ann=FALSE,col="darkblue",lwd=1)
legend("topright",legend = c('Measured','Model'),col = c("darkblue","red"),lty = c(0, 1), lwd=2, pch =c(16, NA))

error=resid(model)
```

### Using $r^2$ to measure model fit

 One important way to assess how well the model fits is to measure the value of $r^2$, where $r$ is the correlation coefficient.
 
```{r ,  echo=TRUE}
summ$r.squared
```

### Noting the conditions

- **The residuals have a normal distribution with mean zero $\varepsilon_i \backsim N(0,\sigma^2)$.**
 
```{r , echo=TRUE}
summary(error)
sd(error)

shapiro.test(error)

hist(error,col="#ADD8E6",main="Histogram of the Residuals",xlab="Residuals")

boxplot(error,main="Residuals",col="#ADD8E6",horizontal=TRUE)

qqnorm(error,pch=19,col="darkblue",main="Normal Probability Plot of the Residuals",bty = "l")
qqline(error,col="red")
```

 As the condition of normality is met, then residual plot lots of residuals close to zero. The residuals also occur at random some above the line, some below the line.

- **The residuals have the same variance for each fitted (predicted) value of $\hat{y}$, $Var(\varepsilon_i)=\sigma^2$.**

```{r , echo=TRUE}
var(error)
```

- **The residuals are independent (don’t affect each other).**

```{r , echo=TRUE}
plot(error,pch=19,col="darkblue",main="Residuals versus the Order of the Data",xlab="Observation Order",ylab="Residual",bty = "l")
par(new=TRUE)
plot(error,type="l",axes=FALSE,ann=FALSE)
abline(h=0,lty=2,col="red") 
legend("topright",legend = c('residual','median'),col = c("darkblue","red"),lty = c(0,2), lwd=2, pch =c(16,NA))
```

# Writing into JSON file

```{r , echo=TRUE}
# creating the list
list1 <- vector(mode="list", length=2)
list1[[1]] <- c("Intercept", "u", "u^2", "u^3")
list1[[2]] <- c(as.numeric(func[1]), as.numeric(func[2]),as.numeric(func[3]), as.numeric(func[4]))

# creating the data for JSON file
jsonData <- toJSON(list1)

# writing into JSON file
write(jsonData, "JSON/motor1.json") 

# Give the created file name to the function
motor1 <- fromJSON(file = "JSON/motor1.json")

# Print the result
print(motor1)

list2 <- vector(mode="list", length=1)
list2[[1]] <- error
jsonData <- toJSON(list2)
write(jsonData, "JSON/motor1_resi.json")

list3 <- vector(mode="list", length=1)
list3[[1]] <- modelagem[,3]
jsonData <- toJSON(list3)
write(jsonData, "JSON/motor1_median.json")
```

# Reference
```{r , echo=TRUE}
#Citing
citation()
citation("readr")
citation("rjson")
citation("latex2exp")

### lm {stats}	R Documentation Fitting Linear Models
#Author(s)
#The design was inspired by the S function of the same name described in Chambers (1992). The implementation of model formula #by Ross Ihaka was based on Wilkinson & Rogers (1973).

### References
## Chambers, J. M. (1992) Linear models. Chapter 4 of Statistical Models in S eds J. M. Chambers and T. J. Hastie, Wadsworth & Brooks/Cole.
##@article{chambers1992linear,
##  title={Linear models. Chapter 4 of statistical models in S},
##  author={Chambers, JM and Hastie, TJ},
##  journal={Wadsworth \& Brooks/Cole},
##  volume={1992},
##  year={1992}
##}

## Wilkinson, G. N. and Rogers, C. E. (1973). Symbolic descriptions of factorial models for analysis of variance. 
##@article{wilkinson1973symbolic,
##  title={Symbolic description of factorial models for analysis of variance},
##  author={Wilkinson, GN and Rogers, CE},
##  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
##  volume={22},
##  number={3},
##  pages={392--399},
##  year={1973},
##  publisher={Wiley Online Library}
##}

## Intermediate Statistics For Dummies - 2007
##@book{rumsey2007intermediate,
##  title={Intermediate statistics for dummies},
##  author={Rumsey, Deborah J},
##  year={2007},
##  publisher={John Wiley \& Sons}
##}
```
